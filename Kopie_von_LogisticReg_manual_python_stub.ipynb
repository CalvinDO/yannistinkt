{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmURzPPRtODn"
      },
      "source": [
        "# Logistic Regression Assignment (due 26 November)\n",
        "\n",
        "In this practical you will learn how to apply logistic regression to the task of predicting two digits from the MNIST database: http://yann.lecun.com/exdb/mnist/. The database contains 60000 train images containing digits and 10000 test images. The images are of size 28 Ã— 28. We will use the images in a vectorized form: a vector of size of 784. The code extracting the digits 0 and 1 is provided in the stubs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQkzOf0dFpxc"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI0dT0-WEYuh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqiqzu3dFXKj",
        "outputId": "7d81c1e5-fa86-4ec4-b2f6-32e45e82904b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSjE8NINEvVO"
      },
      "outputs": [],
      "source": [
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybQxEwHnGsgY"
      },
      "outputs": [],
      "source": [
        "def data_preprocess(images, labels):\n",
        "\n",
        "    # number of examples m  \n",
        "    m = images.shape[0]\n",
        "    \n",
        "    # create vector of ones to concatenate to our data matrix (for intercept terms)\n",
        "    ones = np.ones(shape=[m, 1])\n",
        "    images = np.concatenate((ones, images), axis=1)\n",
        "    \n",
        "    # to retrieve the images and corresponding labels where the label is either 0 or 1, \n",
        "    # we define two logical vectors that can be used to subset our data_matrices\n",
        "    logical_mask_0 = labels == 0\n",
        "    logical_mask_1 = labels == 1\n",
        "    \n",
        "    images_zeros = images[logical_mask_0]\n",
        "    labels_zeros = labels[logical_mask_0]\n",
        "    images_ones = images[logical_mask_1]\n",
        "    labels_ones = labels[logical_mask_1]\n",
        "    \n",
        "    X = np.concatenate((images_zeros, images_ones), axis=0)\n",
        "    y = np.concatenate((labels_zeros, labels_ones), axis=0)\n",
        "    \n",
        "    # shuffle the data and corresponding labels in unison\n",
        "    def _shuffle_in_unison(a, b):\n",
        "        assert len(a) == len(b)\n",
        "        p = np.random.permutation(len(a))\n",
        "        return a[p], b[p]\n",
        "\n",
        "    return _shuffle_in_unison(X,y)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrkpG7BYI8MT"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHEiLlEnNhIm",
        "outputId": "c9e6978e-f390-44d4-d159-ec19ab2995d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print (x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb5Hr9aENMTy",
        "outputId": "764fea8f-c059-4920-f932-819528066c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784)\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.reshape([60000,784])\n",
        "x_test = x_test.reshape([10000,784])\n",
        "print(x_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMi47AaKcHzQ",
        "outputId": "dc2a2c08-5bb2-4687-d946-95c1b5d29bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape:  (12665, 785)\n",
            "shape:  (12665,)\n"
          ]
        }
      ],
      "source": [
        "X,y = data_preprocess(x_train, y_train)\n",
        "print('shape: ', X.shape)\n",
        "print('shape: ', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTx9l6kePGbf"
      },
      "source": [
        "Define hyperparams: learning rate and gradient descent steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzE8JqGXdBy2"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.00001\n",
        "gdc_steps = 100\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5ujHEHrQDy_"
      },
      "source": [
        "Initialize your parameters W\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnXlpZqMR4jP"
      },
      "outputs": [],
      "source": [
        "# number of features n\n",
        "n = X.shape[1]\n",
        "# we need to define our model parameters to be learned. we use W (weights) instead of theta this time.\n",
        "\n",
        "# mean and standard deviation\n",
        "mu = 0\n",
        "sigma = 0.01\n",
        "w = np.random.normal(mu, sigma, n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b-fVYzzR6_r",
        "outputId": "2535aac6-8c5e-4672-c6a6-3c18d01e6a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12665, 785) (785,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape, w.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp3Uc6qUSIi0"
      },
      "source": [
        "Define the sigmoid function, your code here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_gdiFglSMYN"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m14UBJJ6SRGS"
      },
      "source": [
        "Define the loss function as provided in equation 12 (Logistic regression slides)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyCliB0USThU"
      },
      "outputs": [],
      "source": [
        "def compute_cross_entropy_loss(y, y_hat):\n",
        "    return -(y @ np.log(y_hat) + (1 - y_hat) @ np.log(1 - y_hat))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vusNro8SX5-"
      },
      "source": [
        "Start optimization. During training you minimize the loss function. In every iteration your loss should decrease. You also want to look how many correct predictions you have at every iteration. Reminder: the belonging to class digit 1 is when your prediction, $\\hat y$ is greater or equal to 0.5. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKXM0YdDcajm"
      },
      "source": [
        "When you test your prediction vector (containing zero and ones) with the labels (also zero and ones) you can use the equal function. \n",
        "\n",
        "Example:\n",
        "prediction = (1, 0, 1, 1) and the true labels are y = (0, 0, 1, 0).\n",
        "\n",
        "When you test on equality you get following result: correct = (0, 1, 1, 0). Your accuracy is: 0+1+1+0\n",
        "4 = 0.5.\n",
        "You compute the accuracy for the training and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ToLLOp2Scv0",
        "outputId": "3bdc8970-0093-4608-9d9d-522d1e823f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing step 0 of gradient descent.\n",
            "Loss at step 0: 8937.261934070782\n",
            "Accuracy at step 0: [0.83292538]\n",
            "Performing step 1 of gradient descent.\n",
            "Loss at step 1: 6992.11299594477\n",
            "Accuracy at step 1: [0.99060403]\n",
            "Performing step 2 of gradient descent.\n",
            "Loss at step 2: 5565.675332985458\n",
            "Accuracy at step 2: [0.99557837]\n",
            "Performing step 3 of gradient descent.\n",
            "Loss at step 3: 4714.030449118674\n",
            "Accuracy at step 3: [0.99605211]\n",
            "Performing step 4 of gradient descent.\n",
            "Loss at step 4: 4127.733584149572\n",
            "Accuracy at step 4: [0.9958942]\n",
            "Performing step 5 of gradient descent.\n",
            "Loss at step 5: 3694.985544045834\n",
            "Accuracy at step 5: [0.99613107]\n",
            "Performing step 6 of gradient descent.\n",
            "Loss at step 6: 3360.2179435845933\n",
            "Accuracy at step 6: [0.99613107]\n",
            "Performing step 7 of gradient descent.\n",
            "Loss at step 7: 3092.2174909340224\n",
            "Accuracy at step 7: [0.99628899]\n",
            "Performing step 8 of gradient descent.\n",
            "Loss at step 8: 2871.978341926051\n",
            "Accuracy at step 8: [0.99628899]\n",
            "Performing step 9 of gradient descent.\n",
            "Loss at step 9: 2687.2124331406403\n",
            "Accuracy at step 9: [0.99636794]\n",
            "Performing step 10 of gradient descent.\n",
            "Loss at step 10: 2529.5942883400435\n",
            "Accuracy at step 10: [0.99636794]\n",
            "Performing step 11 of gradient descent.\n",
            "Loss at step 11: 2393.2667456033955\n",
            "Accuracy at step 11: [0.99636794]\n",
            "Performing step 12 of gradient descent.\n",
            "Loss at step 12: 2273.979533529312\n",
            "Accuracy at step 12: [0.99636794]\n",
            "Performing step 13 of gradient descent.\n",
            "Loss at step 13: 2168.5673901129235\n",
            "Accuracy at step 13: [0.9964469]\n",
            "Performing step 14 of gradient descent.\n",
            "Loss at step 14: 2074.620554984913\n",
            "Accuracy at step 14: [0.9964469]\n",
            "Performing step 15 of gradient descent.\n",
            "Loss at step 15: 1990.2693257091064\n",
            "Accuracy at step 15: [0.9964469]\n",
            "Performing step 16 of gradient descent.\n",
            "Loss at step 16: 1914.0389092565013\n",
            "Accuracy at step 16: [0.9964469]\n",
            "Performing step 17 of gradient descent.\n",
            "Loss at step 17: 1844.749058761673\n",
            "Accuracy at step 17: [0.99660482]\n",
            "Performing step 18 of gradient descent.\n",
            "Loss at step 18: 1781.443080210553\n",
            "Accuracy at step 18: [0.99660482]\n",
            "Performing step 19 of gradient descent.\n",
            "Loss at step 19: 1723.3365951767432\n",
            "Accuracy at step 19: [0.99668377]\n",
            "Performing step 20 of gradient descent.\n",
            "Loss at step 20: 1669.779894944364\n",
            "Accuracy at step 20: [0.99668377]\n",
            "Performing step 21 of gradient descent.\n",
            "Loss at step 21: 1620.229834394841\n",
            "Accuracy at step 21: [0.99668377]\n",
            "Performing step 22 of gradient descent.\n",
            "Loss at step 22: 1574.228543452842\n",
            "Accuracy at step 22: [0.99668377]\n",
            "Performing step 23 of gradient descent.\n",
            "Loss at step 23: 1531.3870904808218\n",
            "Accuracy at step 23: [0.99668377]\n",
            "Performing step 24 of gradient descent.\n",
            "Loss at step 24: 1491.3727959384537\n",
            "Accuracy at step 24: [0.99668377]\n",
            "Performing step 25 of gradient descent.\n",
            "Loss at step 25: 1453.8992731748667\n",
            "Accuracy at step 25: [0.99668377]\n",
            "Performing step 26 of gradient descent.\n",
            "Loss at step 26: 1418.718531877828\n",
            "Accuracy at step 26: [0.99668377]\n",
            "Performing step 27 of gradient descent.\n",
            "Loss at step 27: 1385.6146593241385\n",
            "Accuracy at step 27: [0.99668377]\n",
            "Performing step 28 of gradient descent.\n",
            "Loss at step 28: 1354.3987211783349\n",
            "Accuracy at step 28: [0.99668377]\n",
            "Performing step 29 of gradient descent.\n",
            "Loss at step 29: 1324.9046140511864\n",
            "Accuracy at step 29: [0.99676273]\n",
            "Performing step 30 of gradient descent.\n",
            "Loss at step 30: 1296.9856674942575\n",
            "Accuracy at step 30: [0.99676273]\n",
            "Performing step 31 of gradient descent.\n",
            "Loss at step 31: 1270.5118410393686\n",
            "Accuracy at step 31: [0.99676273]\n",
            "Performing step 32 of gradient descent.\n",
            "Loss at step 32: 1245.367397372292\n",
            "Accuracy at step 32: [0.99684169]\n",
            "Performing step 33 of gradient descent.\n",
            "Loss at step 33: 1221.4489592614261\n",
            "Accuracy at step 33: [0.99684169]\n",
            "Performing step 34 of gradient descent.\n",
            "Loss at step 34: 1198.6638778910233\n",
            "Accuracy at step 34: [0.99684169]\n",
            "Performing step 35 of gradient descent.\n",
            "Loss at step 35: 1176.9288555034486\n",
            "Accuracy at step 35: [0.99684169]\n",
            "Performing step 36 of gradient descent.\n",
            "Loss at step 36: 1156.1687769711787\n",
            "Accuracy at step 36: [0.99684169]\n",
            "Performing step 37 of gradient descent.\n",
            "Loss at step 37: 1136.3157139885093\n",
            "Accuracy at step 37: [0.99684169]\n",
            "Performing step 38 of gradient descent.\n",
            "Loss at step 38: 1117.3080726452895\n",
            "Accuracy at step 38: [0.99684169]\n",
            "Performing step 39 of gradient descent.\n",
            "Loss at step 39: 1099.0898606987366\n",
            "Accuracy at step 39: [0.99692065]\n",
            "Performing step 40 of gradient descent.\n",
            "Loss at step 40: 1081.6100552494077\n",
            "Accuracy at step 40: [0.99692065]\n",
            "Performing step 41 of gradient descent.\n",
            "Loss at step 41: 1064.8220550192843\n",
            "Accuracy at step 41: [0.99699961]\n",
            "Performing step 42 of gradient descent.\n",
            "Loss at step 42: 1048.6832042238839\n",
            "Accuracy at step 42: [0.99707856]\n",
            "Performing step 43 of gradient descent.\n",
            "Loss at step 43: 1033.154377278419\n",
            "Accuracy at step 43: [0.99707856]\n",
            "Performing step 44 of gradient descent.\n",
            "Loss at step 44: 1018.1996153966411\n",
            "Accuracy at step 44: [0.99707856]\n",
            "Performing step 45 of gradient descent.\n",
            "Loss at step 45: 1003.78580761962\n",
            "Accuracy at step 45: [0.99715752]\n",
            "Performing step 46 of gradient descent.\n",
            "Loss at step 46: 989.8824100197912\n",
            "Accuracy at step 46: [0.99715752]\n",
            "Performing step 47 of gradient descent.\n",
            "Loss at step 47: 976.4611978171658\n",
            "Accuracy at step 47: [0.99715752]\n",
            "Performing step 48 of gradient descent.\n",
            "Loss at step 48: 963.4960459620651\n",
            "Accuracy at step 48: [0.99715752]\n",
            "Performing step 49 of gradient descent.\n",
            "Loss at step 49: 950.9627344155633\n",
            "Accuracy at step 49: [0.99715752]\n",
            "Performing step 50 of gradient descent.\n",
            "Loss at step 50: 938.8387749213848\n",
            "Accuracy at step 50: [0.99715752]\n",
            "Performing step 51 of gradient descent.\n",
            "Loss at step 51: 927.1032565325075\n",
            "Accuracy at step 51: [0.99715752]\n",
            "Performing step 52 of gradient descent.\n",
            "Loss at step 52: 915.7367075489336\n",
            "Accuracy at step 52: [0.99715752]\n",
            "Performing step 53 of gradient descent.\n",
            "Loss at step 53: 904.7209718536234\n",
            "Accuracy at step 53: [0.99715752]\n",
            "Performing step 54 of gradient descent.\n",
            "Loss at step 54: 894.0390979123897\n",
            "Accuracy at step 54: [0.99715752]\n",
            "Performing step 55 of gradient descent.\n",
            "Loss at step 55: 883.6752389394561\n",
            "Accuracy at step 55: [0.99715752]\n",
            "Performing step 56 of gradient descent.\n",
            "Loss at step 56: 873.6145629306665\n",
            "Accuracy at step 56: [0.99715752]\n",
            "Performing step 57 of gradient descent.\n",
            "Loss at step 57: 863.8431714368589\n",
            "Accuracy at step 57: [0.99723648]\n",
            "Performing step 58 of gradient descent.\n",
            "Loss at step 58: 854.348026095558\n",
            "Accuracy at step 58: [0.99731544]\n",
            "Performing step 59 of gradient descent.\n",
            "Loss at step 59: 845.1168820638532\n",
            "Accuracy at step 59: [0.99731544]\n",
            "Performing step 60 of gradient descent.\n",
            "Loss at step 60: 836.1382276024536\n",
            "Accuracy at step 60: [0.99731544]\n",
            "Performing step 61 of gradient descent.\n",
            "Loss at step 61: 827.401229153106\n",
            "Accuracy at step 61: [0.99731544]\n",
            "Performing step 62 of gradient descent.\n",
            "Loss at step 62: 818.8956813311978\n",
            "Accuracy at step 62: [0.99731544]\n",
            "Performing step 63 of gradient descent.\n",
            "Loss at step 63: 810.6119613242279\n",
            "Accuracy at step 63: [0.99731544]\n",
            "Performing step 64 of gradient descent.\n",
            "Loss at step 64: 802.5409872465918\n",
            "Accuracy at step 64: [0.99731544]\n",
            "Performing step 65 of gradient descent.\n",
            "Loss at step 65: 794.6741800530724\n",
            "Accuracy at step 65: [0.99731544]\n",
            "Performing step 66 of gradient descent.\n",
            "Loss at step 66: 787.0034286586913\n",
            "Accuracy at step 66: [0.99731544]\n",
            "Performing step 67 of gradient descent.\n",
            "Loss at step 67: 779.5210579520952\n",
            "Accuracy at step 67: [0.99731544]\n",
            "Performing step 68 of gradient descent.\n",
            "Loss at step 68: 772.2197994242435\n",
            "Accuracy at step 68: [0.99731544]\n",
            "Performing step 69 of gradient descent.\n",
            "Loss at step 69: 765.0927641644837\n",
            "Accuracy at step 69: [0.99739439]\n",
            "Performing step 70 of gradient descent.\n",
            "Loss at step 70: 758.1334180027417\n",
            "Accuracy at step 70: [0.99739439]\n",
            "Performing step 71 of gradient descent.\n",
            "Loss at step 71: 751.3355586000084\n",
            "Accuracy at step 71: [0.99739439]\n",
            "Performing step 72 of gradient descent.\n",
            "Loss at step 72: 744.6932943099811\n",
            "Accuracy at step 72: [0.99739439]\n",
            "Performing step 73 of gradient descent.\n",
            "Loss at step 73: 738.2010246529833\n",
            "Accuracy at step 73: [0.99739439]\n",
            "Performing step 74 of gradient descent.\n",
            "Loss at step 74: 731.8534222594607\n",
            "Accuracy at step 74: [0.99739439]\n",
            "Performing step 75 of gradient descent.\n",
            "Loss at step 75: 725.6454161546794\n",
            "Accuracy at step 75: [0.99739439]\n",
            "Performing step 76 of gradient descent.\n",
            "Loss at step 76: 719.5721762689799\n",
            "Accuracy at step 76: [0.99739439]\n",
            "Performing step 77 of gradient descent.\n",
            "Loss at step 77: 713.6290990692562\n",
            "Accuracy at step 77: [0.99755231]\n",
            "Performing step 78 of gradient descent.\n",
            "Loss at step 78: 707.8117942174198\n",
            "Accuracy at step 78: [0.99755231]\n",
            "Performing step 79 of gradient descent.\n",
            "Loss at step 79: 702.11607217059\n",
            "Accuracy at step 79: [0.99755231]\n",
            "Performing step 80 of gradient descent.\n",
            "Loss at step 80: 696.5379326458044\n",
            "Accuracy at step 80: [0.99755231]\n",
            "Performing step 81 of gradient descent.\n",
            "Loss at step 81: 691.0735538792137\n",
            "Accuracy at step 81: [0.99747335]\n",
            "Performing step 82 of gradient descent.\n",
            "Loss at step 82: 685.7192826161943\n",
            "Accuracy at step 82: [0.99747335]\n",
            "Performing step 83 of gradient descent.\n",
            "Loss at step 83: 680.4716247745628\n",
            "Accuracy at step 83: [0.99747335]\n",
            "Performing step 84 of gradient descent.\n",
            "Loss at step 84: 675.3272367282966\n",
            "Accuracy at step 84: [0.99747335]\n",
            "Performing step 85 of gradient descent.\n",
            "Loss at step 85: 670.2829171638202\n",
            "Accuracy at step 85: [0.99747335]\n",
            "Performing step 86 of gradient descent.\n",
            "Loss at step 86: 665.3355994651323\n",
            "Accuracy at step 86: [0.99747335]\n",
            "Performing step 87 of gradient descent.\n",
            "Loss at step 87: 660.4823445878436\n",
            "Accuracy at step 87: [0.99747335]\n",
            "Performing step 88 of gradient descent.\n",
            "Loss at step 88: 655.7203343856236\n",
            "Accuracy at step 88: [0.99747335]\n",
            "Performing step 89 of gradient descent.\n",
            "Loss at step 89: 651.0468653556479\n",
            "Accuracy at step 89: [0.99747335]\n",
            "Performing step 90 of gradient descent.\n",
            "Loss at step 90: 646.459342772454\n",
            "Accuracy at step 90: [0.99747335]\n",
            "Performing step 91 of gradient descent.\n",
            "Loss at step 91: 641.9552751821375\n",
            "Accuracy at step 91: [0.99747335]\n",
            "Performing step 92 of gradient descent.\n",
            "Loss at step 92: 637.5322692311476\n",
            "Accuracy at step 92: [0.99747335]\n",
            "Performing step 93 of gradient descent.\n",
            "Loss at step 93: 633.188024806009\n",
            "Accuracy at step 93: [0.99747335]\n",
            "Performing step 94 of gradient descent.\n",
            "Loss at step 94: 628.920330462228\n",
            "Accuracy at step 94: [0.99747335]\n",
            "Performing step 95 of gradient descent.\n",
            "Loss at step 95: 624.7270591223474\n",
            "Accuracy at step 95: [0.99747335]\n",
            "Performing step 96 of gradient descent.\n",
            "Loss at step 96: 620.6061640247115\n",
            "Accuracy at step 96: [0.99747335]\n",
            "Performing step 97 of gradient descent.\n",
            "Loss at step 97: 616.5556749059281\n",
            "Accuracy at step 97: [0.99747335]\n",
            "Performing step 98 of gradient descent.\n",
            "Loss at step 98: 612.5736944013374\n",
            "Accuracy at step 98: [0.99747335]\n",
            "Performing step 99 of gradient descent.\n",
            "Loss at step 99: 608.6583946489898\n",
            "Accuracy at step 99: [0.99747335]\n"
          ]
        }
      ],
      "source": [
        "for step in range(0, gdc_steps):\n",
        "    print(\"Performing step \" + str(step) + \" of gradient descent.\")\n",
        "    # perform the dot product between the weights and the examples\n",
        "    z = X @ w\n",
        "\n",
        "    # apply the nonlinearity\n",
        "    y_hat = sigmoid(z)\n",
        "\n",
        "    # normally normalized with -1/m \n",
        "    loss = compute_cross_entropy_loss(y, y_hat)\n",
        "    print(\"Loss at step \" + str(step) + \": \" + str(loss))\n",
        "\n",
        "    # compute the error term\n",
        "    error_term = y_hat - y\n",
        "    \n",
        "    # compute the gradient\n",
        "    gradients = X.T @ error_term\n",
        "\n",
        "    # update w using the gdc update rule\n",
        "    w = w - learning_rate * gradients\n",
        "    \n",
        "    # compute the predictions and cast them to int values\n",
        "    predictions = (y_hat > 0.5).astype(int)\n",
        "\n",
        "    # compute mean accuracy\n",
        "    accuracy = np.sum(np.equal(predictions, y)) / predictions.shape\n",
        "    print(\"Accuracy at step \" + str(step) + \": \" + str(accuracy))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt_XyizgcD7H"
      },
      "source": [
        "Evaluate model on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIysfpyCcIN2",
        "outputId": "2c1f8d6e-257a-4148-fbd1-009b632a0578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_______________________________\n",
            "Starting evaluation of test set\n",
            "Accuracy of test set: [0.99905437]\n"
          ]
        }
      ],
      "source": [
        "print(\"_______________________________\")\n",
        "print(\"Starting evaluation of test set\")\n",
        "\n",
        "X,y = data_preprocess(x_test, y_test)\n",
        "z = X @ w\n",
        "y_hat = sigmoid(z)\n",
        "predictions = (y_hat>0.5).astype(np.int32)\n",
        "accuracy = np.sum(np.equal(predictions, y)) / predictions.shape\n",
        "print(\"Accuracy of test set: \" + str(accuracy))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Kopie von LogisticReg_manual_python_stub.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
